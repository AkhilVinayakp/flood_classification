{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/std_data.csv')\n",
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].apply(lambda x: str(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf5sh = KFold(n_splits= 5, shuffle= True, random_state= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SVC(C=3.0, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "scoring should either be a single string or callable for single metric evaluation or a list/tuple of strings or a dict of scorer name mapped to the callable for multiple metric evaluation. Got array([1.        , 1.        , 1.        , 0.99444444, 0.98888889]) of type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-8e462525c5d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_svm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mkf5sh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m     \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;31m# We clone the estimator to make sure that all the folds are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[1;34m(estimator, scoring)\u001b[0m\n\u001b[0;32m    527\u001b[0m                        for key, scorer in scoring.items()}\n\u001b[0;32m    528\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg_generic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: scoring should either be a single string or callable for single metric evaluation or a list/tuple of strings or a dict of scorer name mapped to the callable for multiple metric evaluation. Got array([1.        , 1.        , 1.        , 0.99444444, 0.98888889]) of type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(model_svm, x, y, cv =kf5sh, scoring=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 0.99444444, 0.98888889])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9966666666666667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________ ** 1 ** ____________________\n",
      "[[26  0  0  0]\n",
      " [ 0 48  0  0]\n",
      " [ 0  0 25  0]\n",
      " [ 0  0  0 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        26\n",
      "        0.19       1.00      1.00      1.00        48\n",
      "         2.5       1.00      1.00      1.00        25\n",
      "         4.5       1.00      1.00      1.00        82\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "1.0\n",
      "________________ ** 2 ** ____________________\n",
      "[[31  0  0  0]\n",
      " [ 0 40  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        31\n",
      "        0.19       1.00      1.00      1.00        40\n",
      "         2.5       1.00      1.00      1.00        27\n",
      "         4.5       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "1.0\n",
      "________________ ** 3 ** ____________________\n",
      "[[31  0  0  0]\n",
      " [ 0 33  0  0]\n",
      " [ 0  0 24  0]\n",
      " [ 0  0  0 92]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        31\n",
      "        0.19       1.00      1.00      1.00        33\n",
      "         2.5       1.00      1.00      1.00        24\n",
      "         4.5       1.00      1.00      1.00        92\n",
      "\n",
      "    accuracy                           1.00       180\n",
      "   macro avg       1.00      1.00      1.00       180\n",
      "weighted avg       1.00      1.00      1.00       180\n",
      "\n",
      "1.0\n",
      "________________ ** 4 ** ____________________\n",
      "[[35  0  0  1]\n",
      " [ 0 46  0  0]\n",
      " [ 0  0 28  0]\n",
      " [ 0  0  0 70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.99        36\n",
      "        0.19       1.00      1.00      1.00        46\n",
      "         2.5       1.00      1.00      1.00        28\n",
      "         4.5       0.99      1.00      0.99        70\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       1.00      0.99      0.99       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n",
      "0.9944444444444445\n",
      "________________ ** 5 ** ____________________\n",
      "[[15  0  0  1]\n",
      " [ 0 47  0  0]\n",
      " [ 0  0 36  0]\n",
      " [ 0  1  0 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.94      0.97        16\n",
      "        0.19       0.98      1.00      0.99        47\n",
      "         2.5       1.00      1.00      1.00        36\n",
      "         4.5       0.99      0.99      0.99        81\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       0.99      0.98      0.99       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n",
      "0.9888888888888889\n",
      "average accuracy :0.9966666666666667\n",
      "\n",
      "\n",
      " classification_report average\n",
      "\n",
      "                    precision    recall    f1-score   \n",
      "\n",
      "         0.00       1.0000      0.9820      0.9920        \n",
      "         0.19       0.9960      1.0000      0.9980        \n",
      "         2.50       1.0000      1.0000      1.0000      \n",
      "         4.50       0.9960      0.9980      0.9960        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "acc_svm = []\n",
    "d0_pre =[]\n",
    "d0_rec=[]\n",
    "d0_f1 =  []\n",
    "d19_pre = []\n",
    "d19_rec =[]\n",
    "d19_f1=[]\n",
    "d25_pre =[]\n",
    "d25_rec=[]\n",
    "d25_f1=[]\n",
    "d45_pre=[]\n",
    "d45_rec =[]\n",
    "d45_f1=[]\n",
    "for train_index, test_index in kf5sh.split(x):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "    model_svm.fit(X_train,Y_train)\n",
    "    y_pred = model_svm.predict(X_test)\n",
    "    print(\"________________ ** {} ** ____________________\".format(i))\n",
    "    print(confusion_matrix(Y_test,y_pred))\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    rep = classification_report(Y_test,y_pred)\n",
    "    print(accuracy_score(Y_test, y_pred))\n",
    "    acc_svm.append(accuracy_score(Y_test, y_pred))\n",
    "    d0_pre.append(float(rep.split('\\n')[2].split('      ')[2].strip()))\n",
    "    d0_rec.append(float(rep.split('\\n')[2].split('      ')[3].strip()))\n",
    "    d0_f1.append(float(rep.split('\\n')[2].split('      ')[4].strip()))\n",
    "    d19_pre.append(float(rep.split('\\n')[3].split('      ')[2].strip()))\n",
    "    d19_rec.append(float(rep.split('\\n')[3].split('      ')[3].strip()))\n",
    "    d19_f1.append(float(rep.split('\\n')[3].split('      ')[4].strip()))\n",
    "    d25_pre.append(float(rep.split('\\n')[4].split('      ')[2].strip()))\n",
    "    d25_rec.append(float(rep.split('\\n')[4].split('      ')[3].strip()))\n",
    "    d25_f1.append(float(rep.split('\\n')[4].split('      ')[4].strip()))\n",
    "    d45_pre.append(float(rep.split('\\n')[5].split('      ')[2].strip()))\n",
    "    d45_rec.append(float(rep.split('\\n')[5].split('      ')[3].strip()))\n",
    "    d45_f1.append(float(rep.split('\\n')[5].split('      ')[4].strip()))\n",
    "    i = i+1\n",
    "    \n",
    "print(\"average accuracy :{}\".format(np.average(acc_svm)))\n",
    "print(\"\\n\\n classification_report average\\n\")\n",
    "print('                    precision    recall    f1-score   \\n\\n         0.00       {:.4f}      {:.4f}      {:.4f}        \\n         0.19       {:.4f}      {:.4f}      {:.4f}        \\n         2.50       {:.4f}      {:.4f}      {:.4f}      \\n         4.50       {:.4f}      {:.4f}      {:.4f}        \\n\\n'.\n",
    "      format(np.average(d0_pre), np.average(d0_rec), np.average(d0_f1), np.average(d19_pre), np.average(d19_rec), np.average(d19_f1), np.average(d25_pre), np.average(d25_rec), np.average(d25_f1), np.average(d45_pre), np.average(d45_rec),np.average(d45_f1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cls_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 1.0, 1.0, 1.0, 1.0], 1.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0_pre, np.average(d0_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 1.0, 1.0, 0.97, 0.94], 0.982)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0_rec, np.average(d0_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 0.99, 0.97]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=3.0, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3.0, kernel='linear')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0  0  1]\n",
      " [ 0 43  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.98        31\n",
      "        0.19       1.00      1.00      1.00        43\n",
      "         2.5       1.00      1.00      1.00        27\n",
      "         4.5       0.99      1.00      0.99        80\n",
      "\n",
      "    accuracy                           0.99       181\n",
      "   macro avg       1.00      0.99      0.99       181\n",
      "weighted avg       0.99      0.99      0.99       181\n",
      "\n",
      "0.994475138121547\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n         0.0       1.00      0.97      0.98        31\\n        0.19       1.00      1.00      1.00        43\\n         2.5       1.00      1.00      1.00        27\\n         4.5       0.99      1.00      0.99        80\\n\\n    accuracy                           0.99       181\\n   macro avg       1.00      0.99      0.99       181\\nweighted avg       0.99      0.99      0.99       181\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '0.46'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-e95bb311ac9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'      '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '0.46'"
     ]
    }
   ],
   "source": [
    "int(classification_report(y_test,y_pred).split('\\n')[5].split('      ')[3].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "f ='3.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '3.5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-15c82ade717a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '3.5'"
     ]
    }
   ],
   "source": [
    "int(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_pre =[]\n",
    "d0_rec=[]\n",
    "d0_f1 =  []\n",
    "d19_pre = []\n",
    "d19_rec =[]\n",
    "d19_f1=[]\n",
    "d25_pre =[]\n",
    "d25_rec=[]\n",
    "d25_f1=[]\n",
    "d45_pre=[]\n",
    "d45_rec =[]\n",
    "d45_f1=[]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
