{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.fftpack as fftpack\n",
    "import sklearn.preprocessing as preprocess\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP\\\\Documents\\\\projects\\\\flood_classification\\\\models'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50 0\n",
      "25 75 1\n",
      "50 100 2\n",
      "75 125 3\n",
      "100 150 4\n",
      "125 175 5\n",
      "150 200 6\n",
      "175 225 7\n",
      "200 250 8\n",
      "225 275 9\n",
      "250 300 10\n",
      "275 325 11\n",
      "300 350 12\n",
      "325 375 13\n",
      "350 400 14\n",
      "375 425 15\n",
      "400 450 16\n",
      "425 475 17\n",
      "450 500 18\n",
      "475 525 19\n",
      "500 550 20\n",
      "525 575 21\n",
      "550 600 22\n",
      "575 625 23\n",
      "600 650 24\n",
      "625 675 25\n",
      "650 700 26\n",
      "675 725 27\n",
      "700 750 28\n",
      "725 775 29\n",
      "750 800 30\n",
      "775 825 31\n",
      "800 850 32\n",
      "825 875 33\n",
      "850 900 34\n",
      "875 925 35\n",
      "900 950 36\n",
      "925 975 37\n",
      "950 1000 38\n",
      "975 1025 39\n",
      "1000 1050 40\n",
      "1025 1075 41\n",
      "1050 1100 42\n",
      "1075 1125 43\n",
      "1100 1150 44\n",
      "1125 1175 45\n",
      "1150 1200 46\n",
      "1175 1225 47\n",
      "1200 1250 48\n",
      "1225 1275 49\n",
      "1250 1300 50\n",
      "1275 1325 51\n",
      "1300 1350 52\n",
      "1325 1375 53\n",
      "1350 1400 54\n",
      "1375 1425 55\n",
      "1400 1450 56\n",
      "1425 1475 57\n",
      "1450 1500 58\n",
      "1475 1525 59\n",
      "1500 1550 60\n",
      "1525 1575 61\n",
      "1550 1600 62\n",
      "1575 1625 63\n",
      "1600 1650 64\n",
      "1625 1675 65\n",
      "1650 1700 66\n",
      "1675 1725 67\n",
      "1700 1750 68\n",
      "1725 1775 69\n",
      "1750 1800 70\n",
      "1775 1825 71\n",
      "1800 1850 72\n",
      "1825 1875 73\n",
      "1850 1900 74\n",
      "1875 1925 75\n",
      "1900 1950 76\n",
      "1925 1975 77\n",
      "1950 2000 78\n",
      "1975 2025 79\n",
      "2000 2050 80\n",
      "2025 2075 81\n",
      "2050 2100 82\n",
      "2075 2125 83\n",
      "2100 2150 84\n",
      "2125 2175 85\n",
      "2150 2200 86\n",
      "2175 2225 87\n",
      "2200 2250 88\n",
      "2225 2275 89\n",
      "2250 2300 90\n",
      "2275 2325 91\n",
      "2300 2350 92\n",
      "2325 2375 93\n",
      "2350 2400 94\n",
      "2375 2425 95\n",
      "2400 2450 96\n",
      "2425 2475 97\n",
      "2450 2500 98\n",
      "2475 2525 99\n",
      "2500 2550 100\n",
      "2525 2575 101\n",
      "2550 2600 102\n",
      "2575 2625 103\n",
      "2600 2650 104\n",
      "2625 2675 105\n",
      "2650 2700 106\n",
      "2675 2725 107\n",
      "2700 2750 108\n",
      "2725 2775 109\n",
      "2750 2800 110\n",
      "2775 2825 111\n",
      "2800 2850 112\n",
      "2825 2875 113\n",
      "2850 2900 114\n",
      "2875 2925 115\n",
      "2900 2950 116\n",
      "2925 2975 117\n",
      "2950 3000 118\n",
      "2975 3025 119\n",
      "3000 3050 120\n",
      "3025 3075 121\n",
      "3050 3100 122\n",
      "3075 3125 123\n",
      "3100 3150 124\n",
      "3125 3175 125\n",
      "3150 3200 126\n",
      "3175 3225 127\n",
      "3200 3250 128\n",
      "3225 3275 129\n",
      "3250 3300 130\n",
      "3275 3325 131\n",
      "3300 3350 132\n",
      "3325 3375 133\n",
      "3350 3400 134\n",
      "3375 3425 135\n",
      "3400 3450 136\n",
      "3425 3475 137\n",
      "3450 3500 138\n",
      "3475 3525 139\n",
      "0 50 140\n",
      "25 75 141\n",
      "50 100 142\n",
      "75 125 143\n",
      "100 150 144\n",
      "125 175 145\n",
      "150 200 146\n",
      "175 225 147\n",
      "200 250 148\n",
      "225 275 149\n",
      "250 300 150\n",
      "275 325 151\n",
      "300 350 152\n",
      "325 375 153\n",
      "350 400 154\n",
      "375 425 155\n",
      "400 450 156\n",
      "425 475 157\n",
      "450 500 158\n",
      "475 525 159\n",
      "500 550 160\n",
      "525 575 161\n",
      "550 600 162\n",
      "575 625 163\n",
      "600 650 164\n",
      "625 675 165\n",
      "650 700 166\n",
      "675 725 167\n",
      "700 750 168\n",
      "725 775 169\n",
      "750 800 170\n",
      "775 825 171\n",
      "800 850 172\n",
      "825 875 173\n",
      "850 900 174\n",
      "875 925 175\n",
      "900 950 176\n",
      "925 975 177\n",
      "950 1000 178\n",
      "975 1025 179\n",
      "1000 1050 180\n",
      "1025 1075 181\n",
      "1050 1100 182\n",
      "1075 1125 183\n",
      "1100 1150 184\n",
      "1125 1175 185\n",
      "1150 1200 186\n",
      "1175 1225 187\n",
      "1200 1250 188\n",
      "1225 1275 189\n",
      "1250 1300 190\n",
      "1275 1325 191\n",
      "1300 1350 192\n",
      "1325 1375 193\n",
      "1350 1400 194\n",
      "1375 1425 195\n",
      "1400 1450 196\n",
      "1425 1475 197\n",
      "1450 1500 198\n",
      "1475 1525 199\n",
      "1500 1550 200\n",
      "1525 1575 201\n",
      "1550 1600 202\n",
      "1575 1625 203\n",
      "1600 1650 204\n",
      "1625 1675 205\n",
      "1650 1700 206\n",
      "1675 1725 207\n",
      "1700 1750 208\n",
      "1725 1775 209\n",
      "1750 1800 210\n",
      "1775 1825 211\n",
      "1800 1850 212\n",
      "1825 1875 213\n",
      "1850 1900 214\n",
      "1875 1925 215\n",
      "1900 1950 216\n",
      "1925 1975 217\n",
      "1950 2000 218\n",
      "1975 2025 219\n",
      "2000 2050 220\n",
      "2025 2075 221\n",
      "2050 2100 222\n",
      "2075 2125 223\n",
      "2100 2150 224\n",
      "2125 2175 225\n",
      "2150 2200 226\n",
      "2175 2225 227\n",
      "2200 2250 228\n",
      "2225 2275 229\n",
      "2250 2300 230\n",
      "2275 2325 231\n",
      "2300 2350 232\n",
      "2325 2375 233\n",
      "2350 2400 234\n",
      "2375 2425 235\n",
      "2400 2450 236\n",
      "2425 2475 237\n",
      "2450 2500 238\n",
      "2475 2525 239\n",
      "2500 2550 240\n",
      "2525 2575 241\n",
      "2550 2600 242\n",
      "2575 2625 243\n",
      "2600 2650 244\n",
      "2625 2675 245\n",
      "2650 2700 246\n",
      "2675 2725 247\n",
      "2700 2750 248\n",
      "2725 2775 249\n",
      "2750 2800 250\n",
      "2775 2825 251\n",
      "2800 2850 252\n",
      "2825 2875 253\n",
      "2850 2900 254\n",
      "2875 2925 255\n",
      "2900 2950 256\n",
      "2925 2975 257\n",
      "2950 3000 258\n",
      "2975 3025 259\n",
      "3000 3050 260\n",
      "3025 3075 261\n",
      "3050 3100 262\n",
      "3075 3125 263\n",
      "3100 3150 264\n",
      "3125 3175 265\n",
      "3150 3200 266\n",
      "3175 3225 267\n",
      "3200 3250 268\n",
      "3225 3275 269\n",
      "3250 3300 270\n",
      "3275 3325 271\n",
      "3300 3350 272\n",
      "3325 3375 273\n",
      "3350 3400 274\n",
      "3375 3425 275\n",
      "3400 3450 276\n",
      "3425 3475 277\n",
      "3450 3500 278\n",
      "3475 3525 279\n",
      "3500 3550 280\n",
      "3525 3575 281\n",
      "3550 3600 282\n",
      "3575 3625 283\n",
      "3600 3650 284\n",
      "3625 3675 285\n",
      "3650 3700 286\n",
      "3675 3725 287\n",
      "3700 3750 288\n",
      "3725 3775 289\n",
      "3750 3800 290\n",
      "3775 3825 291\n",
      "3800 3850 292\n",
      "3825 3875 293\n",
      "3850 3900 294\n",
      "3875 3925 295\n",
      "3900 3950 296\n",
      "3925 3975 297\n",
      "3950 4000 298\n",
      "3975 4025 299\n",
      "4000 4050 300\n",
      "4025 4075 301\n",
      "4050 4100 302\n",
      "4075 4125 303\n",
      "4100 4150 304\n",
      "4125 4175 305\n",
      "4150 4200 306\n",
      "4175 4225 307\n",
      "4200 4250 308\n",
      "4225 4275 309\n",
      "4250 4300 310\n",
      "4275 4325 311\n",
      "4300 4350 312\n",
      "4325 4375 313\n",
      "4350 4400 314\n",
      "4375 4425 315\n",
      "4400 4450 316\n",
      "4425 4475 317\n",
      "4450 4500 318\n",
      "4475 4525 319\n",
      "4500 4550 320\n",
      "4525 4575 321\n",
      "4550 4600 322\n",
      "4575 4625 323\n",
      "4600 4650 324\n",
      "4625 4675 325\n",
      "4650 4700 326\n",
      "4675 4725 327\n",
      "4700 4750 328\n",
      "4725 4775 329\n",
      "4750 4800 330\n",
      "4775 4825 331\n",
      "4800 4850 332\n",
      "4825 4875 333\n",
      "4850 4900 334\n",
      "4875 4925 335\n",
      "4900 4950 336\n",
      "4925 4975 337\n",
      "4950 5000 338\n",
      "4975 5025 339\n",
      "5000 5050 340\n",
      "5025 5075 341\n",
      "5050 5100 342\n",
      "5075 5125 343\n",
      "5100 5150 344\n",
      "5125 5175 345\n",
      "5150 5200 346\n",
      "5175 5225 347\n",
      "5200 5250 348\n",
      "5225 5275 349\n",
      "5250 5300 350\n",
      "5275 5325 351\n",
      "5300 5350 352\n",
      "5325 5375 353\n",
      "0 50 354\n",
      "25 75 355\n",
      "50 100 356\n",
      "75 125 357\n",
      "100 150 358\n",
      "125 175 359\n",
      "150 200 360\n",
      "175 225 361\n",
      "200 250 362\n",
      "225 275 363\n",
      "250 300 364\n",
      "275 325 365\n",
      "300 350 366\n",
      "325 375 367\n",
      "350 400 368\n",
      "375 425 369\n",
      "400 450 370\n",
      "425 475 371\n",
      "450 500 372\n",
      "475 525 373\n",
      "500 550 374\n",
      "525 575 375\n",
      "550 600 376\n",
      "575 625 377\n",
      "600 650 378\n",
      "625 675 379\n",
      "650 700 380\n",
      "675 725 381\n",
      "700 750 382\n",
      "725 775 383\n",
      "750 800 384\n",
      "775 825 385\n",
      "800 850 386\n",
      "825 875 387\n",
      "850 900 388\n",
      "875 925 389\n",
      "900 950 390\n",
      "925 975 391\n",
      "950 1000 392\n",
      "975 1025 393\n",
      "1000 1050 394\n",
      "1025 1075 395\n",
      "1050 1100 396\n",
      "1075 1125 397\n",
      "1100 1150 398\n",
      "1125 1175 399\n",
      "1150 1200 400\n",
      "1175 1225 401\n",
      "1200 1250 402\n",
      "1225 1275 403\n",
      "1250 1300 404\n",
      "1275 1325 405\n",
      "1300 1350 406\n",
      "1325 1375 407\n",
      "1350 1400 408\n",
      "1375 1425 409\n",
      "1400 1450 410\n",
      "1425 1475 411\n",
      "1450 1500 412\n",
      "1475 1525 413\n",
      "1500 1550 414\n",
      "1525 1575 415\n",
      "1550 1600 416\n",
      "1575 1625 417\n",
      "1600 1650 418\n",
      "1625 1675 419\n",
      "1650 1700 420\n",
      "1675 1725 421\n",
      "1700 1750 422\n",
      "1725 1775 423\n",
      "1750 1800 424\n",
      "1775 1825 425\n",
      "1800 1850 426\n",
      "1825 1875 427\n",
      "1850 1900 428\n",
      "1875 1925 429\n",
      "1900 1950 430\n",
      "1925 1975 431\n",
      "1950 2000 432\n",
      "1975 2025 433\n",
      "2000 2050 434\n",
      "2025 2075 435\n",
      "2050 2100 436\n",
      "2075 2125 437\n",
      "2100 2150 438\n",
      "2125 2175 439\n",
      "2150 2200 440\n",
      "2175 2225 441\n",
      "2200 2250 442\n",
      "2225 2275 443\n",
      "2250 2300 444\n",
      "2275 2325 445\n",
      "2300 2350 446\n",
      "2325 2375 447\n",
      "2350 2400 448\n",
      "2375 2425 449\n",
      "2400 2450 450\n",
      "2425 2475 451\n",
      "2450 2500 452\n",
      "2475 2525 453\n",
      "2500 2550 454\n",
      "2525 2575 455\n",
      "2550 2600 456\n",
      "2575 2625 457\n",
      "2600 2650 458\n",
      "2625 2675 459\n",
      "2650 2700 460\n",
      "2675 2725 461\n",
      "2700 2750 462\n",
      "2725 2775 463\n",
      "2750 2800 464\n",
      "2775 2825 465\n",
      "2800 2850 466\n",
      "2825 2875 467\n",
      "2850 2900 468\n",
      "2875 2925 469\n",
      "2900 2950 470\n",
      "2925 2975 471\n",
      "2950 3000 472\n",
      "2975 3025 473\n",
      "3000 3050 474\n",
      "3025 3075 475\n",
      "3050 3100 476\n",
      "3075 3125 477\n",
      "3100 3150 478\n",
      "3125 3175 479\n",
      "3150 3200 480\n",
      "3175 3225 481\n",
      "3200 3250 482\n",
      "3225 3275 483\n",
      "3250 3300 484\n",
      "3275 3325 485\n",
      "3300 3350 486\n",
      "3325 3375 487\n",
      "3350 3400 488\n",
      "3375 3425 489\n",
      "3400 3450 490\n",
      "3425 3475 491\n",
      "3450 3500 492\n",
      "3475 3525 493\n",
      "3500 3550 494\n",
      "3525 3575 495\n",
      "3550 3600 496\n",
      "3575 3625 497\n",
      "3600 3650 498\n",
      "3625 3675 499\n",
      "3650 3700 500\n",
      "3675 3725 501\n",
      "3700 3750 502\n",
      "3725 3775 503\n",
      "3750 3800 504\n",
      "3775 3825 505\n",
      "3800 3850 506\n",
      "3825 3875 507\n",
      "3850 3900 508\n",
      "3875 3925 509\n",
      "3900 3950 510\n",
      "3925 3975 511\n",
      "3950 4000 512\n",
      "3975 4025 513\n",
      "4000 4050 514\n",
      "4025 4075 515\n",
      "4050 4100 516\n",
      "4075 4125 517\n",
      "4100 4150 518\n",
      "4125 4175 519\n",
      "4150 4200 520\n",
      "4175 4225 521\n",
      "4200 4250 522\n",
      "4225 4275 523\n",
      "4250 4300 524\n",
      "4275 4325 525\n",
      "4300 4350 526\n",
      "4325 4375 527\n",
      "4350 4400 528\n",
      "4375 4425 529\n",
      "4400 4450 530\n",
      "4425 4475 531\n",
      "4450 4500 532\n",
      "4475 4525 533\n",
      "4500 4550 534\n",
      "4525 4575 535\n",
      "4550 4600 536\n",
      "4575 4625 537\n",
      "4600 4650 538\n",
      "4625 4675 539\n",
      "4650 4700 540\n",
      "4675 4725 541\n",
      "4700 4750 542\n",
      "4725 4775 543\n",
      "4750 4800 544\n",
      "4775 4825 545\n",
      "4800 4850 546\n",
      "4825 4875 547\n",
      "4850 4900 548\n",
      "4875 4925 549\n",
      "4900 4950 550\n",
      "4925 4975 551\n",
      "4950 5000 552\n",
      "4975 5025 553\n",
      "5000 5050 554\n",
      "5025 5075 555\n",
      "5050 5100 556\n",
      "5075 5125 557\n",
      "5100 5150 558\n",
      "5125 5175 559\n",
      "5150 5200 560\n",
      "5175 5225 561\n",
      "5200 5250 562\n",
      "5225 5275 563\n",
      "5250 5300 564\n",
      "5275 5325 565\n",
      "5300 5350 566\n",
      "5325 5375 567\n",
      "5350 5400 568\n",
      "5375 5425 569\n",
      "5400 5450 570\n",
      "5425 5475 571\n",
      "5450 5500 572\n",
      "5475 5525 573\n",
      "5500 5550 574\n",
      "5525 5575 575\n",
      "5550 5600 576\n",
      "5575 5625 577\n",
      "5600 5650 578\n",
      "5625 5675 579\n",
      "5650 5700 580\n",
      "5675 5725 581\n",
      "5700 5750 582\n",
      "5725 5775 583\n",
      "5750 5800 584\n",
      "5775 5825 585\n",
      "5800 5850 586\n",
      "5825 5875 587\n",
      "5850 5900 588\n",
      "5875 5925 589\n",
      "5900 5950 590\n",
      "5925 5975 591\n",
      "5950 6000 592\n",
      "5975 6025 593\n",
      "6000 6050 594\n",
      "6025 6075 595\n",
      "6050 6100 596\n",
      "6075 6125 597\n",
      "6100 6150 598\n",
      "6125 6175 599\n",
      "6150 6200 600\n",
      "6175 6225 601\n",
      "6200 6250 602\n",
      "6225 6275 603\n",
      "6250 6300 604\n",
      "6275 6325 605\n",
      "6300 6350 606\n",
      "6325 6375 607\n",
      "6350 6400 608\n",
      "6375 6425 609\n",
      "6400 6450 610\n",
      "6425 6475 611\n",
      "6450 6500 612\n",
      "6475 6525 613\n",
      "6500 6550 614\n",
      "6525 6575 615\n",
      "6550 6600 616\n",
      "6575 6625 617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600 6650 618\n",
      "6625 6675 619\n",
      "6650 6700 620\n",
      "6675 6725 621\n",
      "6700 6750 622\n",
      "6725 6775 623\n",
      "6750 6800 624\n",
      "6775 6825 625\n",
      "6800 6850 626\n",
      "6825 6875 627\n",
      "6850 6900 628\n",
      "6875 6925 629\n",
      "6900 6950 630\n",
      "6925 6975 631\n",
      "6950 7000 632\n",
      "6975 7025 633\n",
      "7000 7050 634\n",
      "7025 7075 635\n",
      "7050 7100 636\n",
      "7075 7125 637\n",
      "7100 7150 638\n",
      "7125 7175 639\n",
      "7150 7200 640\n",
      "7175 7225 641\n",
      "7200 7250 642\n",
      "7225 7275 643\n",
      "7250 7300 644\n",
      "7275 7325 645\n",
      "7300 7350 646\n",
      "7325 7375 647\n",
      "7350 7400 648\n",
      "7375 7425 649\n",
      "7400 7450 650\n",
      "7425 7475 651\n",
      "7450 7500 652\n",
      "7475 7525 653\n",
      "7500 7550 654\n",
      "7525 7575 655\n",
      "7550 7600 656\n",
      "7575 7625 657\n",
      "7600 7650 658\n",
      "7625 7675 659\n",
      "7650 7700 660\n",
      "7675 7725 661\n",
      "7700 7750 662\n",
      "7725 7775 663\n",
      "7750 7800 664\n",
      "7775 7825 665\n",
      "7800 7850 666\n",
      "7825 7875 667\n",
      "7850 7900 668\n",
      "7875 7925 669\n",
      "7900 7950 670\n",
      "7925 7975 671\n",
      "7950 8000 672\n",
      "7975 8025 673\n",
      "8000 8050 674\n",
      "8025 8075 675\n",
      "8050 8100 676\n",
      "8075 8125 677\n",
      "8100 8150 678\n",
      "8125 8175 679\n",
      "8150 8200 680\n",
      "8175 8225 681\n",
      "8200 8250 682\n",
      "8225 8275 683\n",
      "8250 8300 684\n",
      "8275 8325 685\n",
      "8300 8350 686\n",
      "8325 8375 687\n",
      "8350 8400 688\n",
      "8375 8425 689\n",
      "8400 8450 690\n",
      "8425 8475 691\n",
      "8450 8500 692\n",
      "8475 8525 693\n",
      "8500 8550 694\n",
      "8525 8575 695\n",
      "8550 8600 696\n",
      "8575 8625 697\n",
      "8600 8650 698\n",
      "8625 8675 699\n",
      "8650 8700 700\n",
      "8675 8725 701\n",
      "8700 8750 702\n",
      "8725 8775 703\n",
      "8750 8800 704\n",
      "8775 8825 705\n",
      "8800 8850 706\n",
      "8825 8875 707\n",
      "8850 8900 708\n",
      "8875 8925 709\n",
      "8900 8950 710\n",
      "8925 8975 711\n",
      "8950 9000 712\n",
      "8975 9025 713\n",
      "9000 9050 714\n",
      "9025 9075 715\n",
      "9050 9100 716\n",
      "9075 9125 717\n",
      "9100 9150 718\n",
      "9125 9175 719\n",
      "9150 9200 720\n",
      "9175 9225 721\n",
      "9200 9250 722\n",
      "9225 9275 723\n",
      "9250 9300 724\n",
      "9275 9325 725\n",
      "9300 9350 726\n",
      "9325 9375 727\n",
      "9350 9400 728\n",
      "9375 9425 729\n",
      "9400 9450 730\n",
      "9425 9475 731\n",
      "9450 9500 732\n",
      "9475 9525 733\n",
      "9500 9550 734\n",
      "9525 9575 735\n",
      "9550 9600 736\n",
      "9575 9625 737\n",
      "9600 9650 738\n",
      "9625 9675 739\n",
      "9650 9700 740\n",
      "9675 9725 741\n",
      "9700 9750 742\n",
      "9725 9775 743\n",
      "9750 9800 744\n",
      "9775 9825 745\n",
      "9800 9850 746\n",
      "9825 9875 747\n",
      "9850 9900 748\n",
      "9875 9925 749\n",
      "9900 9950 750\n",
      "9925 9975 751\n",
      "9950 10000 752\n",
      "9975 10025 753\n",
      "10000 10050 754\n",
      "10025 10075 755\n",
      "10050 10100 756\n",
      "10075 10125 757\n",
      "10100 10150 758\n",
      "10125 10175 759\n",
      "10150 10200 760\n",
      "10175 10225 761\n",
      "0 50 762\n",
      "25 75 763\n",
      "50 100 764\n",
      "75 125 765\n",
      "100 150 766\n",
      "125 175 767\n",
      "150 200 768\n",
      "175 225 769\n",
      "200 250 770\n",
      "225 275 771\n",
      "250 300 772\n",
      "275 325 773\n",
      "300 350 774\n",
      "325 375 775\n",
      "350 400 776\n",
      "375 425 777\n",
      "400 450 778\n",
      "425 475 779\n",
      "450 500 780\n",
      "475 525 781\n",
      "500 550 782\n",
      "525 575 783\n",
      "550 600 784\n",
      "575 625 785\n",
      "600 650 786\n",
      "625 675 787\n",
      "650 700 788\n",
      "675 725 789\n",
      "700 750 790\n",
      "725 775 791\n",
      "750 800 792\n",
      "775 825 793\n",
      "800 850 794\n",
      "825 875 795\n",
      "850 900 796\n",
      "875 925 797\n",
      "900 950 798\n",
      "925 975 799\n",
      "950 1000 800\n",
      "975 1025 801\n",
      "1000 1050 802\n",
      "1025 1075 803\n",
      "1050 1100 804\n",
      "1075 1125 805\n",
      "1100 1150 806\n",
      "1125 1175 807\n",
      "1150 1200 808\n",
      "1175 1225 809\n",
      "1200 1250 810\n",
      "1225 1275 811\n",
      "1250 1300 812\n",
      "1275 1325 813\n",
      "1300 1350 814\n",
      "1325 1375 815\n",
      "1350 1400 816\n",
      "1375 1425 817\n",
      "1400 1450 818\n",
      "1425 1475 819\n",
      "1450 1500 820\n",
      "1475 1525 821\n",
      "1500 1550 822\n",
      "1525 1575 823\n",
      "1550 1600 824\n",
      "1575 1625 825\n",
      "1600 1650 826\n",
      "1625 1675 827\n",
      "1650 1700 828\n",
      "1675 1725 829\n",
      "1700 1750 830\n",
      "1725 1775 831\n",
      "1750 1800 832\n",
      "1775 1825 833\n",
      "1800 1850 834\n",
      "1825 1875 835\n",
      "1850 1900 836\n",
      "1875 1925 837\n",
      "1900 1950 838\n",
      "1925 1975 839\n",
      "1950 2000 840\n",
      "1975 2025 841\n",
      "2000 2050 842\n",
      "2025 2075 843\n",
      "2050 2100 844\n",
      "2075 2125 845\n",
      "2100 2150 846\n",
      "2125 2175 847\n",
      "2150 2200 848\n",
      "2175 2225 849\n",
      "2200 2250 850\n",
      "2225 2275 851\n",
      "2250 2300 852\n",
      "2275 2325 853\n",
      "2300 2350 854\n",
      "2325 2375 855\n",
      "2350 2400 856\n",
      "2375 2425 857\n",
      "2400 2450 858\n",
      "2425 2475 859\n",
      "2450 2500 860\n",
      "2475 2525 861\n",
      "2500 2550 862\n",
      "2525 2575 863\n",
      "2550 2600 864\n",
      "2575 2625 865\n",
      "2600 2650 866\n",
      "2625 2675 867\n",
      "2650 2700 868\n",
      "2675 2725 869\n",
      "2700 2750 870\n",
      "2725 2775 871\n",
      "2750 2800 872\n",
      "2775 2825 873\n",
      "2800 2850 874\n",
      "2825 2875 875\n",
      "2850 2900 876\n",
      "2875 2925 877\n",
      "2900 2950 878\n",
      "2925 2975 879\n",
      "2950 3000 880\n",
      "2975 3025 881\n",
      "3000 3050 882\n",
      "3025 3075 883\n",
      "3050 3100 884\n",
      "3075 3125 885\n",
      "3100 3150 886\n",
      "3125 3175 887\n",
      "3150 3200 888\n",
      "3175 3225 889\n",
      "3200 3250 890\n",
      "3225 3275 891\n",
      "3250 3300 892\n",
      "3275 3325 893\n",
      "3300 3350 894\n",
      "3325 3375 895\n",
      "3350 3400 896\n",
      "3375 3425 897\n",
      "3400 3450 898\n",
      "3425 3475 899\n",
      "3450 3500 900\n",
      "3475 3525 901\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Sep 15 21:19:11 2020\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.fftpack as fftpack\n",
    "\n",
    "# setting the root directory as the corrent working directory\n",
    "os.chdir(\"C:\\\\Users\\\\HP\\\\Documents\\\\projects\\\\flood_classification\")\n",
    "data = pd.read_csv(os.path.join(\"data\",\"filt_data.csv\"))\n",
    "'''\n",
    "50 consecutive\n",
    "readings when data is recorded at 10Hz, with an overlap\n",
    "of 25 readings between consecutive examples\n",
    "\n",
    "window_size = 50\n",
    "overlap = 25\n",
    "\n",
    "since overlap is set to 25 per window then total number of windows for \n",
    "a class c will be c/25\n",
    "\n",
    "'''\n",
    "def call_ftt(frame):\n",
    "    fft = fftpack.fft(np.array(frame))\n",
    "    amplitudes = np.abs(fft)\n",
    "    coe = amplitudes[1:6]\n",
    "    return np.sum(coe)\n",
    "\n",
    "\n",
    "def energy(frame):\n",
    "    fft = fftpack.fft(np.array(frame))\n",
    "    amplitudes = np.abs(fft)\n",
    "    coe = amplitudes[1:26]\n",
    "    power = coe ** 2\n",
    "    return np.sum(power)/25\n",
    "\n",
    "\n",
    "labels = [0,0.19,4.5,2.5]\n",
    "\n",
    "#creating an empty dataframe to store new data\n",
    "a = np.zeros(shape=(902,46))\n",
    "nd = pd.DataFrame(a, columns = [\n",
    "        'gx_mean','gx_median','gx_variance','gx_fft','gx_spec_energy',\n",
    "        'gy_mean','gy_median','gy_variance','gy_fft','gy_spec_energy',\n",
    "        'gz_mean','gz_median','gz_variance','gz_fft','gz_spec_energy',\n",
    "        'ax_mean','ax_median','ax_variance','ax_fft','ax_spec_energy',\n",
    "        'ay_mean','ay_median','ay_variance','ay_fft','ay_spec_energy',\n",
    "        'az_mean','az_median','az_variance','az_fft','az_spec_energy',\n",
    "        'wx_mean','wx_median','wx_variance','wx_fft','wx_spec_energy',\n",
    "        'wy_mean','wy_median','wy_variance','wy_fft','wy_spec_energy',\n",
    "        'wz_mean','wz_median','wz_variance','wz_fft','wz_spec_energy',\n",
    "        'label'\n",
    "        ])\n",
    "\n",
    "k = 0 # keeping the row index of the new dataFrame\n",
    "\n",
    "# defining the functions to calculate the sum of coefficients of fft\n",
    "# done above\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "for l in labels:\n",
    "    dl = data[data[\"label\"]== l].iloc[:,:-1]\n",
    "    row_max = dl.shape[0]\n",
    "    # assigning the highest multiple of 50 less than row_max\n",
    "    fn_row = row_max - row_max % 50\n",
    "    i, j = 0, 50\n",
    "    while(True):\n",
    "        print(i,j,k)\n",
    "#         print(k)\n",
    "        \n",
    "        for col in dl.columns:\n",
    "            nd[col+\"_mean\"][k] = dl[col].iloc[i:j].mean()\n",
    "            #print(nd[col+\"_mean\"][k] )\n",
    "            nd[col+\"_median\"][k] = dl[col].iloc[i:j].median()\n",
    "            nd[col+\"_variance\"][k] = dl[col].iloc[i:j].var()\n",
    "            nd[col+\"_fft\"][k] = call_ftt(dl[col].iloc[i:j].values)\n",
    "            nd[col+\"_spec_energy\"][k] = energy(dl[col].iloc[i:j].values)\n",
    "#             print(np.array(dl[col].iloc[i:j]))\n",
    "            nd['label'][k] = l\n",
    "            \n",
    "        \n",
    "        i = i + 25\n",
    "        j = j + 25\n",
    "        k = k + 1\n",
    "        if i >= fn_row:\n",
    "            break\n",
    "        \n",
    "nd.to_csv(os.path.join(\"data\",\"win_data_ext.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dP = pd.read_csv(os.path.join(\"data\",\"win_data_ext.csv\"))\n",
    "dP['label'] = dP['label'].apply(lambda x: str(x))\n",
    "features = dP.columns[:-1]\n",
    "for i in features:\n",
    "    dP[i] = preprocess.scale(dP[i])\n",
    "dP.to_csv(os.path.join(\"data\",\"std_data_ext.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(\"data\",\"std_data_ext.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].apply(lambda x: str(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "kf5sh = KFold(n_splits= 5, shuffle= True, random_state= 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = (SVC(C=3.0, kernel='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________ ** 1 ** ____________________\n",
      "[[26  0  0  0]\n",
      " [ 0 48  0  0]\n",
      " [ 0  0 25  0]\n",
      " [ 0  0  0 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        26\n",
      "        0.19       1.00      1.00      1.00        48\n",
      "         2.5       1.00      1.00      1.00        25\n",
      "         4.5       1.00      1.00      1.00        82\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "1.0\n",
      "________________ ** 2 ** ____________________\n",
      "[[31  0  0  0]\n",
      " [ 0 40  0  0]\n",
      " [ 2  0 25  0]\n",
      " [ 0  0  0 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97        31\n",
      "        0.19       1.00      1.00      1.00        40\n",
      "         2.5       1.00      0.93      0.96        27\n",
      "         4.5       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           0.99       181\n",
      "   macro avg       0.98      0.98      0.98       181\n",
      "weighted avg       0.99      0.99      0.99       181\n",
      "\n",
      "0.988950276243094\n",
      "________________ ** 3 ** ____________________\n",
      "[[31  0  0  0]\n",
      " [ 0 33  0  0]\n",
      " [ 0  0 24  0]\n",
      " [ 0  0  0 92]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        31\n",
      "        0.19       1.00      1.00      1.00        33\n",
      "         2.5       1.00      1.00      1.00        24\n",
      "         4.5       1.00      1.00      1.00        92\n",
      "\n",
      "    accuracy                           1.00       180\n",
      "   macro avg       1.00      1.00      1.00       180\n",
      "weighted avg       1.00      1.00      1.00       180\n",
      "\n",
      "1.0\n",
      "________________ ** 4 ** ____________________\n",
      "[[35  0  0  1]\n",
      " [ 0 46  0  0]\n",
      " [ 0  0 28  0]\n",
      " [ 0  0  0 70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.99        36\n",
      "        0.19       1.00      1.00      1.00        46\n",
      "         2.5       1.00      1.00      1.00        28\n",
      "         4.5       0.99      1.00      0.99        70\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       1.00      0.99      0.99       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n",
      "0.9944444444444445\n",
      "________________ ** 5 ** ____________________\n",
      "[[15  0  0  1]\n",
      " [ 0 47  0  0]\n",
      " [ 1  0 35  0]\n",
      " [ 0  0  0 81]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94        16\n",
      "        0.19       1.00      1.00      1.00        47\n",
      "         2.5       1.00      0.97      0.99        36\n",
      "         4.5       0.99      1.00      0.99        81\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       0.98      0.98      0.98       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n",
      "0.9888888888888889\n",
      "average accuracy :0.9944567219152856\n",
      "\n",
      "\n",
      " classification_report average\n",
      "\n",
      "                    precision    recall    f1-score   \n",
      "\n",
      "         0.00       0.9760      0.9820      0.9800        \n",
      "         0.19       1.0000      1.0000      1.0000        \n",
      "         2.50       1.0000      0.9800      0.9900      \n",
      "         4.50       0.9960      1.0000      0.9960        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying the 5 fold cross validation\n",
    "i = 1\n",
    "acc_svm = []\n",
    "d0_pre =[]\n",
    "d0_rec=[]\n",
    "d0_f1 =  []\n",
    "d19_pre = []\n",
    "d19_rec =[]\n",
    "d19_f1=[]\n",
    "d25_pre =[]\n",
    "d25_rec=[]\n",
    "d25_f1=[]\n",
    "d45_pre=[]\n",
    "d45_rec =[]\n",
    "d45_f1=[]\n",
    "for train_index, test_index in kf5sh.split(x):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "    model_svm.fit(X_train,Y_train)\n",
    "    y_pred = model_svm.predict(X_test)\n",
    "    print(\"________________ ** {} ** ____________________\".format(i))\n",
    "    print(confusion_matrix(Y_test,y_pred))\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    print(accuracy_score(Y_test, y_pred))\n",
    "    acc_svm.append(accuracy_score(Y_test, y_pred))\n",
    "    rep = classification_report(Y_test,y_pred)\n",
    "    d0_pre.append(float(rep.split('\\n')[2].split('      ')[2].strip()))\n",
    "    d0_rec.append(float(rep.split('\\n')[2].split('      ')[3].strip()))\n",
    "    d0_f1.append(float(rep.split('\\n')[2].split('      ')[4].strip()))\n",
    "    d19_pre.append(float(rep.split('\\n')[3].split('      ')[2].strip()))\n",
    "    d19_rec.append(float(rep.split('\\n')[3].split('      ')[3].strip()))\n",
    "    d19_f1.append(float(rep.split('\\n')[3].split('      ')[4].strip()))\n",
    "    d25_pre.append(float(rep.split('\\n')[4].split('      ')[2].strip()))\n",
    "    d25_rec.append(float(rep.split('\\n')[4].split('      ')[3].strip()))\n",
    "    d25_f1.append(float(rep.split('\\n')[4].split('      ')[4].strip()))\n",
    "    d45_pre.append(float(rep.split('\\n')[5].split('      ')[2].strip()))\n",
    "    d45_rec.append(float(rep.split('\\n')[5].split('      ')[3].strip()))\n",
    "    d45_f1.append(float(rep.split('\\n')[5].split('      ')[4].strip()))\n",
    "    i = i+1\n",
    "    \n",
    "print(\"average accuracy :{}\".format(np.average(acc_svm)))\n",
    "print(\"\\n\\n classification_report average\\n\")\n",
    "print('                    precision    recall    f1-score   \\n\\n         0.00       {:.4f}      {:.4f}      {:.4f}        \\n         0.19       {:.4f}      {:.4f}      {:.4f}        \\n         2.50       {:.4f}      {:.4f}      {:.4f}      \\n         4.50       {:.4f}      {:.4f}      {:.4f}        \\n\\n'.\n",
    "      format(np.average(d0_pre), np.average(d0_rec), np.average(d0_f1), np.average(d19_pre), np.average(d19_rec), np.average(d19_f1), np.average(d25_pre), np.average(d25_rec), np.average(d25_f1), np.average(d45_pre), np.average(d45_rec),np.average(d45_f1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=30,criterion='entropy',max_depth=10,max_features='sqrt',max_leaf_nodes=None,min_samples_split=8, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________ ** 1 ** ____________________\n",
      "[[26  0  0  0]\n",
      " [ 0 48  0  0]\n",
      " [ 0  0 25  0]\n",
      " [ 0  0  0 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        26\n",
      "        0.19       1.00      1.00      1.00        48\n",
      "         2.5       1.00      1.00      1.00        25\n",
      "         4.5       1.00      1.00      1.00        82\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "1.0\n",
      "________________ ** 2 ** ____________________\n",
      "[[31  0  0  0]\n",
      " [ 0 40  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        31\n",
      "        0.19       1.00      1.00      1.00        40\n",
      "         2.5       1.00      1.00      1.00        27\n",
      "         4.5       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           1.00       181\n",
      "   macro avg       1.00      1.00      1.00       181\n",
      "weighted avg       1.00      1.00      1.00       181\n",
      "\n",
      "1.0\n",
      "________________ ** 3 ** ____________________\n",
      "[[31  0  0  0]\n",
      " [ 0 33  0  0]\n",
      " [ 0  0 24  0]\n",
      " [ 0  0  0 92]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        31\n",
      "        0.19       1.00      1.00      1.00        33\n",
      "         2.5       1.00      1.00      1.00        24\n",
      "         4.5       1.00      1.00      1.00        92\n",
      "\n",
      "    accuracy                           1.00       180\n",
      "   macro avg       1.00      1.00      1.00       180\n",
      "weighted avg       1.00      1.00      1.00       180\n",
      "\n",
      "1.0\n",
      "________________ ** 4 ** ____________________\n",
      "[[36  0  0  0]\n",
      " [ 0 46  0  0]\n",
      " [ 0  0 28  0]\n",
      " [ 0  1  0 69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        36\n",
      "        0.19       0.98      1.00      0.99        46\n",
      "         2.5       1.00      1.00      1.00        28\n",
      "         4.5       1.00      0.99      0.99        70\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       0.99      1.00      1.00       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n",
      "0.9944444444444445\n",
      "________________ ** 5 ** ____________________\n",
      "[[16  0  0  0]\n",
      " [ 0 46  0  1]\n",
      " [ 0  1 35  0]\n",
      " [ 0  0  0 81]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        16\n",
      "        0.19       0.98      0.98      0.98        47\n",
      "         2.5       1.00      0.97      0.99        36\n",
      "         4.5       0.99      1.00      0.99        81\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       0.99      0.99      0.99       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n",
      "0.9888888888888889\n",
      "average accuracy :0.9966666666666667\n",
      "\n",
      "\n",
      " classification_report average\n",
      "\n",
      "                    precision    recall    f1-score   \n",
      "\n",
      "         0.00       1.0000      1.0000      1.0000        \n",
      "         0.19       0.9920      0.9960      0.9940        \n",
      "         2.50       1.0000      0.9940      0.9980      \n",
      "         4.50       0.9980      0.9980      0.9960        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying the 5 fold cross validation\n",
    "i = 1\n",
    "acc_rf = []\n",
    "d0_pre =[]\n",
    "d0_rec=[]\n",
    "d0_f1 =  []\n",
    "d19_pre = []\n",
    "d19_rec =[]\n",
    "d19_f1=[]\n",
    "d25_pre =[]\n",
    "d25_rec=[]\n",
    "d25_f1=[]\n",
    "d45_pre=[]\n",
    "d45_rec =[]\n",
    "d45_f1=[]\n",
    "for train_index, test_index in kf5sh.split(x):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "    model_rf.fit(X_train,Y_train)\n",
    "    y_pred = model_rf.predict(X_test)\n",
    "    print(\"________________ ** {} ** ____________________\".format(i))\n",
    "    print(confusion_matrix(Y_test,y_pred))\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    print(accuracy_score(Y_test, y_pred))\n",
    "    acc_rf.append(accuracy_score(Y_test, y_pred))\n",
    "    rep = classification_report(Y_test,y_pred)\n",
    "    d0_pre.append(float(rep.split('\\n')[2].split('      ')[2].strip()))\n",
    "    d0_rec.append(float(rep.split('\\n')[2].split('      ')[3].strip()))\n",
    "    d0_f1.append(float(rep.split('\\n')[2].split('      ')[4].strip()))\n",
    "    d19_pre.append(float(rep.split('\\n')[3].split('      ')[2].strip()))\n",
    "    d19_rec.append(float(rep.split('\\n')[3].split('      ')[3].strip()))\n",
    "    d19_f1.append(float(rep.split('\\n')[3].split('      ')[4].strip()))\n",
    "    d25_pre.append(float(rep.split('\\n')[4].split('      ')[2].strip()))\n",
    "    d25_rec.append(float(rep.split('\\n')[4].split('      ')[3].strip()))\n",
    "    d25_f1.append(float(rep.split('\\n')[4].split('      ')[4].strip()))\n",
    "    d45_pre.append(float(rep.split('\\n')[5].split('      ')[2].strip()))\n",
    "    d45_rec.append(float(rep.split('\\n')[5].split('      ')[3].strip()))\n",
    "    d45_f1.append(float(rep.split('\\n')[5].split('      ')[4].strip()))\n",
    "    i = i+1\n",
    "    \n",
    "print(\"average accuracy :{}\".format(np.average(acc_rf)))\n",
    "print(\"\\n\\n classification_report average\\n\")\n",
    "print('                    precision    recall    f1-score   \\n\\n         0.00       {:.4f}      {:.4f}      {:.4f}        \\n         0.19       {:.4f}      {:.4f}      {:.4f}        \\n         2.50       {:.4f}      {:.4f}      {:.4f}      \\n         4.50       {:.4f}      {:.4f}      {:.4f}        \\n\\n'.\n",
    "      format(np.average(d0_pre), np.average(d0_rec), np.average(d0_f1), np.average(d19_pre), np.average(d19_rec), np.average(d19_f1), np.average(d25_pre), np.average(d25_rec), np.average(d25_f1), np.average(d45_pre), np.average(d45_rec),np.average(d45_f1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NA√èVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb =  GaussianNB(priors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________ ** 1 ** ____________________\n",
      "[[26  0  0  0]\n",
      " [ 0 48  0  0]\n",
      " [ 0  0 25  0]\n",
      " [ 0  2  0 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        26\n",
      "        0.19       0.96      1.00      0.98        48\n",
      "         2.5       1.00      1.00      1.00        25\n",
      "         4.5       1.00      0.98      0.99        82\n",
      "\n",
      "    accuracy                           0.99       181\n",
      "   macro avg       0.99      0.99      0.99       181\n",
      "weighted avg       0.99      0.99      0.99       181\n",
      "\n",
      "0.988950276243094\n",
      "________________ ** 2 ** ____________________\n",
      "[[31  0  0  0]\n",
      " [ 2 38  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 4  2  0 77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      1.00      0.91        31\n",
      "        0.19       0.95      0.95      0.95        40\n",
      "         2.5       1.00      1.00      1.00        27\n",
      "         4.5       1.00      0.93      0.96        83\n",
      "\n",
      "    accuracy                           0.96       181\n",
      "   macro avg       0.95      0.97      0.96       181\n",
      "weighted avg       0.96      0.96      0.96       181\n",
      "\n",
      "0.9558011049723757\n",
      "________________ ** 3 ** ____________________\n",
      "[[30  0  1  0]\n",
      " [ 0 33  0  0]\n",
      " [ 0  0 24  0]\n",
      " [ 1  2  0 89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97        31\n",
      "        0.19       0.94      1.00      0.97        33\n",
      "         2.5       0.96      1.00      0.98        24\n",
      "         4.5       1.00      0.97      0.98        92\n",
      "\n",
      "    accuracy                           0.98       180\n",
      "   macro avg       0.97      0.98      0.98       180\n",
      "weighted avg       0.98      0.98      0.98       180\n",
      "\n",
      "0.9777777777777777\n",
      "________________ ** 4 ** ____________________\n",
      "[[36  0  0  0]\n",
      " [ 2 44  0  0]\n",
      " [ 0  0 28  0]\n",
      " [ 0  2  0 68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97        36\n",
      "        0.19       0.96      0.96      0.96        46\n",
      "         2.5       1.00      1.00      1.00        28\n",
      "         4.5       1.00      0.97      0.99        70\n",
      "\n",
      "    accuracy                           0.98       180\n",
      "   macro avg       0.98      0.98      0.98       180\n",
      "weighted avg       0.98      0.98      0.98       180\n",
      "\n",
      "0.9777777777777777\n",
      "________________ ** 5 ** ____________________\n",
      "[[15  0  1  0]\n",
      " [ 1 46  0  0]\n",
      " [ 0  0 36  0]\n",
      " [ 1  1  0 79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.94      0.91        16\n",
      "        0.19       0.98      0.98      0.98        47\n",
      "         2.5       0.97      1.00      0.99        36\n",
      "         4.5       1.00      0.98      0.99        81\n",
      "\n",
      "    accuracy                           0.98       180\n",
      "   macro avg       0.96      0.97      0.97       180\n",
      "weighted avg       0.98      0.98      0.98       180\n",
      "\n",
      "0.9777777777777777\n",
      "average accuracy :0.9756169429097605\n",
      "\n",
      "\n",
      " classification_report average\n",
      "\n",
      "                    precision    recall    f1-score   \n",
      "\n",
      "         0.00       0.9280      0.9820      0.9520        \n",
      "         0.19       0.9580      0.9780      0.9680        \n",
      "         2.50       0.9860      1.0000      0.9940      \n",
      "         4.50       1.0000      0.9660      0.9820        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "acc_nb = []\n",
    "d0_pre =[]\n",
    "d0_rec=[]\n",
    "d0_f1 =  []\n",
    "d19_pre = []\n",
    "d19_rec =[]\n",
    "d19_f1=[]\n",
    "d25_pre =[]\n",
    "d25_rec=[]\n",
    "d25_f1=[]\n",
    "d45_pre=[]\n",
    "d45_rec =[]\n",
    "d45_f1=[]\n",
    "for train_index, test_index in kf5sh.split(x):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "    model_nb.fit(X_train,Y_train)\n",
    "    y_pred = model_nb.predict(X_test)\n",
    "    print(\"________________ ** {} ** ____________________\".format(i))\n",
    "    print(confusion_matrix(Y_test,y_pred))\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    print(accuracy_score(Y_test, y_pred))\n",
    "    acc_nb.append(accuracy_score(Y_test, y_pred))\n",
    "    rep = classification_report(Y_test,y_pred)\n",
    "    d0_pre.append(float(rep.split('\\n')[2].split('      ')[2].strip()))\n",
    "    d0_rec.append(float(rep.split('\\n')[2].split('      ')[3].strip()))\n",
    "    d0_f1.append(float(rep.split('\\n')[2].split('      ')[4].strip()))\n",
    "    d19_pre.append(float(rep.split('\\n')[3].split('      ')[2].strip()))\n",
    "    d19_rec.append(float(rep.split('\\n')[3].split('      ')[3].strip()))\n",
    "    d19_f1.append(float(rep.split('\\n')[3].split('      ')[4].strip()))\n",
    "    d25_pre.append(float(rep.split('\\n')[4].split('      ')[2].strip()))\n",
    "    d25_rec.append(float(rep.split('\\n')[4].split('      ')[3].strip()))\n",
    "    d25_f1.append(float(rep.split('\\n')[4].split('      ')[4].strip()))\n",
    "    d45_pre.append(float(rep.split('\\n')[5].split('      ')[2].strip()))\n",
    "    d45_rec.append(float(rep.split('\\n')[5].split('      ')[3].strip()))\n",
    "    d45_f1.append(float(rep.split('\\n')[5].split('      ')[4].strip()))\n",
    "    i = i+1\n",
    "    \n",
    "print(\"average accuracy :{}\".format(np.average(acc_nb)))\n",
    "print(\"\\n\\n classification_report average\\n\")\n",
    "print('                    precision    recall    f1-score   \\n\\n         0.00       {:.4f}      {:.4f}      {:.4f}        \\n         0.19       {:.4f}      {:.4f}      {:.4f}        \\n         2.50       {:.4f}      {:.4f}      {:.4f}      \\n         4.50       {:.4f}      {:.4f}      {:.4f}        \\n\\n'.\n",
    "      format(np.average(d0_pre), np.average(d0_rec), np.average(d0_f1), np.average(d19_pre), np.average(d19_rec), np.average(d19_f1), np.average(d25_pre), np.average(d25_rec), np.average(d25_f1), np.average(d45_pre), np.average(d45_rec),np.average(d45_f1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________ACCURACY TABLE______________________________________________________\n",
      "||________________________SVM__________________||____________________0.9944567219152856____________________||\n",
      "||__________________RANDOM_FOREST______________||____________________0.9966666666666667____________________||\n",
      "||________________________NB___________________||____________________0.9756169429097605____________________||\n"
     ]
    }
   ],
   "source": [
    "print(\"_______________________________________ACCURACY TABLE______________________________________________________\")\n",
    "print(\"||________________________SVM__________________||____________________{}____________________||\".format(np.average(acc_svm)))\n",
    "print(\"||__________________RANDOM_FOREST______________||____________________{}____________________||\".format(np.average(acc_rf)))\n",
    "print(\"||________________________NB___________________||____________________{}____________________||\".format(np.average(acc_nb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
